{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP1: Read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T14:13:46.072310Z",
     "start_time": "2022-05-06T14:13:43.601775Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T14:13:59.089065Z",
     "start_time": "2022-05-06T14:13:59.075524Z"
    }
   },
   "outputs": [],
   "source": [
    "def readData(dataset_type=\"train\"):\n",
    "\n",
    "    events_f = open(\"./data/{}.data.txt\".format(dataset_type),encoding = \"utf-8\")        \n",
    "    list_of_events = [line.strip('\\n') for line in events_f.readlines()]\n",
    "    #get a list of crawled tweet ids\n",
    "    events_f.close()\n",
    "    if dataset_type == \"test\":\n",
    "        crawl_tobjs = [i.split(\".\")[0] for i in os.listdir(\"./data/tweet-objects\")]\n",
    "    else:\n",
    "        crawl_tobjs = [i.split(\".\")[0] for i in os.listdir(\"./data/crawlData/{}_objs\".format(dataset_type))]\n",
    "    \n",
    "    event_dir_list=[] #[[event0:{obj},{obj}...],[event1:]...[eventn:]]\n",
    "    # function created to get value, convert str to datetime format \n",
    "    def get_create_time(tweet):\n",
    "        create_at = tweet.get('created_at')\n",
    "        try:\n",
    "            to_datetime = datetime.datetime.strptime(create_at, '%Y-%m-%dT%H:%M:%S.000Z')\n",
    "        except ValueError:\n",
    "            # Sat Apr 04 17:00:40 +0000 2020\n",
    "            to_datetime = datetime.datetime.strptime(create_at, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "        return to_datetime\n",
    "        \n",
    "    remove_eve_indexs = []\n",
    "    for event in list_of_events:\n",
    "        each_id_lists = event.split(\",\")\n",
    "        merge_tweets = []\n",
    "        # if the source tweet does not exit, we can assume that the event makes no sense for predicting.\n",
    "        if each_id_lists[0] in crawl_tobjs:\n",
    "\n",
    "            for tid in each_id_lists:\n",
    "                if tid in crawl_tobjs:\n",
    "                    if dataset_type == \"test\":\n",
    "                        f = open(\"./data/tweet-objects/{}.json\".format(tid))\n",
    "                    else:\n",
    "                        f = open(\"./data/crawlData/{}_objs/{}.json\".format(dataset_type,tid))\n",
    "                    merge_tweets.append(json.load(f))\n",
    "                    f.close()\n",
    "        else:\n",
    "            remove_eve_indexs.append(list_of_events.index(event))\n",
    "            continue\n",
    "\n",
    "        #sorting the tweets by create_at time, the tweet would be replied based on the context of previous one\n",
    "        event_dir_list.append(sorted(merge_tweets, key=get_create_time))\n",
    "    \n",
    "    if dataset_type not in [\"test\",\"covid\"]:                \n",
    "        labels_f = open(\"./data/{}.label.txt\".format(dataset_type),encoding = \"utf-8\")\n",
    "        labels_list = [0 if label.strip(\"\\n\") == \"nonrumour\" else 1 for label in labels_f.readlines()]\n",
    "        labels_f.close()\n",
    "        event_labels = [labels_list[i] for i in range(len(labels_list)) if i not in remove_eve_indexs]\n",
    "        return event_dir_list,event_labels\n",
    "    elif dataset_type == \"covid\":\n",
    "        return event_dir_list, remove_eve_indexs\n",
    "    else:\n",
    "        return event_dir_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP2: Preprocessing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T11:07:54.821764Z",
     "start_time": "2022-05-07T11:07:53.086932Z"
    }
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset_type=\"train\"):\n",
    "    event_labels = []\n",
    "    if dataset_type in [\"train\",\"dev\"]:\n",
    "        event_dir_list,event_labels = readData(dataset_type)\n",
    "    elif dataset_type == \"covid\":\n",
    "        event_dir_list, line_index = readData(dataset_type)\n",
    "    else:\n",
    "        event_dir_list = readData(dataset_type)\n",
    "        \n",
    "    #load BERT's WordPiece tokenisation model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "    \n",
    "    #Creating an attention mask - For actual tokens its set to 1, for padding tokens its set to 0\n",
    "    def create_attention_masks(input_ids):\n",
    "        attention_masks = []\n",
    "        for seq in input_ids:\n",
    "            # containing 1s for no padded tokens\n",
    "            seq_mask = [float(i>0) for i in seq]\n",
    "            attention_masks.append(seq_mask)\n",
    "        return np.array(attention_masks)\n",
    "    \n",
    "    def textNormalized(text,lang='en'):\n",
    "        #convert unicode to emoji and then get its description\n",
    "        expanded_text = contractions.fix(text)\n",
    "        text_emoticon = expanded_text.encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "        for i in text_emoticon:\n",
    "            if i in emoji.UNICODE_EMOJI[lang]:\n",
    "                text_emoticon = re.sub(i, emoji.UNICODE_EMOJI['en'][i], text_emoticon)\n",
    "        url_replace = re.sub(r'http\\S+', 'http', text_emoticon)\n",
    "        #mention_replace = re.sub(r'@[a-z|0-9]+', '@user', url_replace)\n",
    "        \n",
    "        return url_replace\n",
    "    \n",
    "    input_ids_list = []\n",
    "    attn_masks_list = []\n",
    "    sentence_tokens = []\n",
    "    max_len = 0\n",
    "    for event in event_dir_list:\n",
    "        event_df = pd.DataFrame.from_dict(event, orient='columns') \n",
    "        event_df['text'] = event_df['text'].transform(textNormalized)\n",
    "        event_df['text'].drop_duplicates()\n",
    "        for each in list(event_df['text']):\n",
    "            if len(each.split(\" \")) > max_len:\n",
    "                max_len = len(each.split(\" \"))\n",
    "        \n",
    "        tokens_list=[]\n",
    "        for index, row in event_df.iterrows():\n",
    "            if index == 0 :\n",
    "                texts = ['[AUT]']\n",
    "            else:\n",
    "                texts = ['[REP]']\n",
    "            if row['user']['verified']=='true':\n",
    "                texts.append('[VER]')\n",
    "            else:\n",
    "                texts.append('[NON]')\n",
    "            user_mentions = row['entities']['user_mentions']\n",
    "            text = row['text']\n",
    "            if user_mentions  != [] :\n",
    "                \n",
    "                for i in user_mentions:\n",
    "                    text = text.replace('@'+i['screen_name'],'[MEN]'+ i['name'])\n",
    "                                        \n",
    "            texts.extend(tokenizer.tokenize(text)+['[SEP]'])\n",
    "            tokens_list.append(texts)\n",
    "\n",
    "        #texts = list(event_df['text'].drop_duplicates())\n",
    "        #tokens_list = list(map(lambda t: tokenizer.tokenize(t)+['[SEP]'], texts))\n",
    "\n",
    "        merge_tokens = [ele for sub_list in tokens_list for ele in sub_list]\n",
    "        #tokens_without_sw = [word for word in merge_tokens if not word in stopwords.words()]\n",
    "        #lemmatizer = WordNetLemmatizer()\n",
    "        #lem_tokens = list(map(lambda t:lemmatizer.lemmatize(t),tokens_without_sw))\n",
    "        #print(merge_tokens)        \n",
    "        sentence_tokens.append(['[CLS]'] + merge_tokens)  \n",
    "    #tokenizer_texts = [tokens + ['[PAD]' for n in range(MAX_LEN - len(tokens))] if len(tokens) < MAX_LEN else tokens[:MAX_LEN-1] + ['[SEP]'] for tokens in tokenizer_texts]\n",
    "    #Converting Input words to corresponding ids\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in sentence_tokens]\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(sequences = input_ids,maxlen=128,dtype='long',padding='post',truncating='post',value=0)\n",
    "    attn_masks = create_attention_masks(input_ids)    \n",
    "    return input_ids,event_labels,attn_masks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP3-a: Loading train/dev/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T09:55:57.243224Z",
     "start_time": "2022-05-05T09:54:58.342185Z"
    }
   },
   "outputs": [],
   "source": [
    "train_input_ids, train_labels,train_attn_masks = preprocessing(dataset_type=\"train\")\n",
    "dev_input_ids, dev_labels, dev_attn_masks = preprocessing(dataset_type=\"dev\")\n",
    "test_input_ids, test_labels, test_attn_masks = preprocessing(dataset_type=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP3-b: Balancing dataset (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1244, 1: 318})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 420, 1: 115})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids_list = train_input_ids.tolist()\n",
    "train_attn_masks_list = train_attn_masks.tolist()\n",
    "train_df = pd.DataFrame({'input_ids':train_input_ids_list,'labels':train_labels,'attn_masks':train_attn_masks_list})\n",
    "train_df.to_csv(\"train_preprocessing.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_input_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bal_train_indexs = sample(list(train_df[train_df['labels']==0].index),318)\n",
    "bal_train_indexs += list(train_df[train_df['labels']==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_train_df = train_df.loc[bal_train_indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_input_ids_list = dev_input_ids.tolist()\n",
    "dev_attn_masks_list = dev_attn_masks.tolist()\n",
    "dev_df = pd.DataFrame({'input_ids':dev_input_ids_list,'labels':dev_labels,'attn_masks':dev_attn_masks_list})\n",
    "dev_df.to_csv(\"dev_preprocessing.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_dev_indexs = sample(list(dev_df[dev_df['labels']==0].index),115)\n",
    "bal_dev_indexs += list(dev_df[dev_df['labels']==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_dev_df = dev_df.loc[bal_dev_indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids_list = test_input_ids.tolist()\n",
    "test_attn_masks_list = test_attn_masks.tolist()\n",
    "test_df = pd.DataFrame({'input_ids':test_input_ids_list,'attn_masks':test_attn_masks_list})\n",
    "test_df.to_csv(\"test_preprocessing.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# STEP4: Feeding data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If BALANCED (unexpected performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(list(bal_train_df['input_ids']))\n",
    "X_val = torch.tensor(list(bal_dev_df['input_ids']))\n",
    "y_train = torch.tensor(list(bal_train_df['labels'] ))\n",
    "y_val = torch.tensor(list(bal_dev_df['labels']))\n",
    "attention_masks_train = torch.tensor(list(bal_train_df['attn_masks']))\n",
    "attention_masks_val = torch.tensor(list(bal_dev_df['attn_masks']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(train_input_ids)\n",
    "X_val = torch.tensor(dev_input_ids)\n",
    "y_train = torch.tensor(train_labels )\n",
    "y_val = torch.tensor(dev_labels)\n",
    "attention_masks_train = torch.tensor(train_attn_masks)\n",
    "attention_masks_val = torch.tensor(dev_attn_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = torch.tensor(test_input_ids)\n",
    "test_attention_masks = torch.tensor(test_attn_masks)\n",
    "#Dataset wrapping tensors.\n",
    "train_data = torch.utils.data.TensorDataset(X_train, attention_masks_train, y_train)\n",
    "val_data = torch.utils.data.TensorDataset(X_val, attention_masks_val, y_val)\n",
    "test_data = torch.utils.data.TensorDataset(test_input_ids, test_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Samples elements randomly. If without replacement(default), then sample from a shuffled dataset.\n",
    "train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_data)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP5: Building RumorDetectionClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertForSequenceClassification\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option1: (APPLIED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T10:17:22.740059Z",
     "start_time": "2022-05-05T10:17:22.729676Z"
    }
   },
   "outputs": [],
   "source": [
    "class RumorDetectionClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RumorDetectionClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        #Classification layer\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        \n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myBertModel():\n",
    "    x_train, y_train,x_details = readData(dataset_type=\"train\",feature=\"text\")\n",
    "    \n",
    "    # Bert layers\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text') \n",
    "    \n",
    "    def get_sentence_embedding(sentences):\n",
    "    \n",
    "        bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "        bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
    "\n",
    "        #keys:['input_word_ids', 'input_mask', 'input_type_ids']\n",
    "        preprocessed_text = bert_preprocess(sentences)\n",
    "        #keys: ['sequence_output', 'encoder_outputs', 'default', 'pooled_output']\n",
    "        bert_results = bert_encoder(preprocessed_text)['pooled_output']\n",
    "        return bert_results\n",
    "    \n",
    "    outputs = get_sentence_embedding(text_input)\n",
    "    # Neural network layers\n",
    "    l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs)\n",
    "    l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "\n",
    "    # Use inputs and outputs to construct a final model\n",
    "    model = tf.keras.Model(inputs=[text_input], outputs = [l])\n",
    "    \n",
    "    METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "    model.fit(x_train, y_train, epochs=2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP6: Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss() #cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --Hyper_parameter by Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Optimization for Hugging Face Transformers\n",
    "#https://wandb.ai/amogkam/transformers/reports/Hyperparameter-Optimization-for-Hugging-Face-Transformers--VmlldzoyMTc2ODI#grid-search-(baseline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#command shells\n",
    "#wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/mist/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: syunmbgv\n",
      "Sweep URL: https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid', \n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "\n",
    "        'learning_rate': {\n",
    "            'values': [ 5e-5, 3e-5 ]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [8,16,32]\n",
    "        },\n",
    "        'epochs':{\n",
    "            'values':[4]\n",
    "        },\n",
    "        'model_name_or_path':{\n",
    "            'values': [\"bert-base-uncased\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_defaults = {\n",
    "            'learning_rate': 3e-5,\n",
    "       \n",
    "            'batch_size': 32,\n",
    "\n",
    "            'epochs':4\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "import datetime\n",
    "def ret_dataloader():\n",
    "    batch_size = wandb.config.batch_size\n",
    "    print('batch_size = ', batch_size)\n",
    "    train_dataloader = DataLoader(\n",
    "                train_data,  # The training samples.\n",
    "                sampler = train_sampler, \n",
    "                batch_size = batch_size # Trains with this batch size.\n",
    "            )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "                val_data, # The validation samples.\n",
    "                sampler = val_sampler, # Pull out batches sequentially.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            )\n",
    "    return train_dataloader,validation_dataloader\n",
    "def ret_optim(model):\n",
    "    print('Learning_rate = ',wandb.config.learning_rate )\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                      lr = wandb.config.learning_rate\n",
    "                    )\n",
    "    return optimizer\n",
    "\n",
    "def ret_scheduler(train_dataloader,optimizer):\n",
    "    epochs = wandb.config.epochs\n",
    "    print('epochs =>', epochs)\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "    return scheduler\n",
    "\n",
    "def get_accuracy(logits, labels):\n",
    "    probs = torch.sigmoid(torch.tensor(logits)) \n",
    "    #probs = F.softmax(logits, dim=0)\n",
    "    preds = (probs > 0.5).long()\n",
    "    acc = f1_score(preds, labels, average = 'micro')\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train():\n",
    "    wandb.login()\n",
    "    wandb.init()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    model = RumorDetectionClassifier()\n",
    "    model.to(device)\n",
    "    #wandb.init(config=sweep_defaults)\n",
    "    train_dataloader,val_dataloader = ret_dataloader()\n",
    "    optimizer = ret_optim(model)\n",
    "    scheduler = ret_scheduler(train_dataloader,optimizer)\n",
    "\n",
    "    #print(\"config \",wandb.config.learning_rate, \"\\n\",wandb.config)\n",
    "    seed_val = 42\n",
    "   \n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    \n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    epochs = wandb.config.epochs\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Always clear any previously calculated gradients before performing a backward pass. \n",
    "            model.zero_grad()        \n",
    "\n",
    "            logits = model(b_input_ids, b_input_mask)\n",
    "            loss = criterion(logits.squeeze(-1), b_labels.float())\n",
    "            \n",
    "            wandb.log({'train_batch_loss':loss.item()})\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        wandb.log({'avg_train_loss':avg_train_loss})\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in val_dataloader:\n",
    "\n",
    "            b_input_ids = batch[0].cuda()\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                logits = model(b_input_ids, b_input_mask)\n",
    "                loss = criterion(logits.squeeze(-1), b_labels.float())\n",
    "                \n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            total_eval_accuracy += get_accuracy(logits, label_ids)\n",
    "            \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "        \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        wandb.log({'val_accuracy':avg_val_accuracy,'avg_val_loss':avg_val_loss})\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rax7ox6q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name_or_path: bert-base-uncased\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzanqraaa21\u001b[0m (\u001b[33mnlp_pj\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mist/wandb/run-20220512_134941-rax7ox6q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp_pj/uncategorized/runs/rax7ox6q\" target=\"_blank\">logical-sweep-1</a></strong> to <a href=\"https://wandb.ai/nlp_pj/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv\" target=\"_blank\">https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  8\n",
      "Learning_rate =  5e-05\n",
      "epochs => 4\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    196.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    196.    Elapsed: 0:00:08.\n",
      "  Batch   120  of    196.    Elapsed: 0:00:12.\n",
      "  Batch   160  of    196.    Elapsed: 0:00:16.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epcoh took: 0:00:19\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 0.22\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    196.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    196.    Elapsed: 0:00:07.\n",
      "  Batch   120  of    196.    Elapsed: 0:00:11.\n",
      "  Batch   160  of    196.    Elapsed: 0:00:15.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 0.25\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    196.    Elapsed: 0:00:03.\n",
      "  Batch    80  of    196.    Elapsed: 0:00:07.\n",
      "  Batch   120  of    196.    Elapsed: 0:00:10.\n",
      "  Batch   160  of    196.    Elapsed: 0:00:13.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 0.26\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    196.    Elapsed: 0:00:03.\n",
      "  Batch    80  of    196.    Elapsed: 0:00:07.\n",
      "  Batch   120  of    196.    Elapsed: 0:00:11.\n",
      "  Batch   160  of    196.    Elapsed: 0:00:14.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.25\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:16 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>avg_val_loss</td><td>▁▇█▆</td></tr><tr><td>train_batch_loss</td><td>▅▅▄▄▅▄▃█▂▇▂▁▆▁▃▇▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.01068</td></tr><tr><td>avg_val_loss</td><td>0.24851</td></tr><tr><td>train_batch_loss</td><td>0.0006</td></tr><tr><td>val_accuracy</td><td>0.94776</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">logical-sweep-1</strong>: <a href=\"https://wandb.ai/nlp_pj/uncategorized/runs/rax7ox6q\" target=\"_blank\">https://wandb.ai/nlp_pj/uncategorized/runs/rax7ox6q</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220512_134941-rax7ox6q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cb7etou3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name_or_path: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mist/wandb/run-20220512_135126-cb7etou3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp_pj/uncategorized/runs/cb7etou3\" target=\"_blank\">good-sweep-2</a></strong> to <a href=\"https://wandb.ai/nlp_pj/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv\" target=\"_blank\">https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  8\n",
      "Learning_rate =  3e-05\n",
      "epochs => 4\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    196.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    196.    Elapsed: 0:00:08.\n",
      "  Batch   120  of    196.    Elapsed: 0:00:11.\n",
      "  Batch   160  of    196.    Elapsed: 0:00:15.\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 0.26\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    196.    Elapsed: 0:00:03.\n",
      "  Batch    80  of    196.    Elapsed: 0:00:07.\n",
      "  Batch   120  of    196.    Elapsed: 0:00:11.\n",
      "  Batch   160  of    196.    Elapsed: 0:00:15.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation Loss: 0.27\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    196.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    196.    Elapsed: 0:00:07.\n",
      "  Batch   120  of    196.    Elapsed: 0:00:11.\n",
      "  Batch   160  of    196.    Elapsed: 0:00:15.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.28\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    196.    Elapsed: 0:00:03.\n",
      "  Batch    80  of    196.    Elapsed: 0:00:07.\n",
      "  Batch   120  of    196.    Elapsed: 0:00:10.\n",
      "  Batch   160  of    196.    Elapsed: 0:00:13.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.25\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:15 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>avg_val_loss</td><td>▅▆█▁</td></tr><tr><td>train_batch_loss</td><td>▂▃▂▃▃▂▂▃▁▂▄▁▁▁▃▃▁▂█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.01227</td></tr><tr><td>avg_val_loss</td><td>0.24517</td></tr><tr><td>train_batch_loss</td><td>0.00066</td></tr><tr><td>val_accuracy</td><td>0.95336</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">good-sweep-2</strong>: <a href=\"https://wandb.ai/nlp_pj/uncategorized/runs/cb7etou3\" target=\"_blank\">https://wandb.ai/nlp_pj/uncategorized/runs/cb7etou3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220512_135126-cb7etou3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n1m9lna0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name_or_path: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mist/wandb/run-20220512_135310-n1m9lna0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp_pj/uncategorized/runs/n1m9lna0\" target=\"_blank\">graceful-sweep-3</a></strong> to <a href=\"https://wandb.ai/nlp_pj/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv\" target=\"_blank\">https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  16\n",
      "Learning_rate =  5e-05\n",
      "epochs => 4\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     98.    Elapsed: 0:00:04.\n",
      "  Batch    80  of     98.    Elapsed: 0:00:09.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epcoh took: 0:00:11\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 0.28\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     98.    Elapsed: 0:00:04.\n",
      "  Batch    80  of     98.    Elapsed: 0:00:09.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epcoh took: 0:00:10\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 0.24\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     98.    Elapsed: 0:00:04.\n",
      "  Batch    80  of     98.    Elapsed: 0:00:09.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epcoh took: 0:00:10\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 0.23\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     98.    Elapsed: 0:00:04.\n",
      "  Batch    80  of     98.    Elapsed: 0:00:08.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:00:10\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.22\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:46 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>avg_val_loss</td><td>█▄▂▁</td></tr><tr><td>train_batch_loss</td><td>█▆▅▇▅▄▅▄▃▆▃▂▁▁▃▅▁▁▄▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.00809</td></tr><tr><td>avg_val_loss</td><td>0.22025</td></tr><tr><td>train_batch_loss</td><td>0.00099</td></tr><tr><td>val_accuracy</td><td>0.94853</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">graceful-sweep-3</strong>: <a href=\"https://wandb.ai/nlp_pj/uncategorized/runs/n1m9lna0\" target=\"_blank\">https://wandb.ai/nlp_pj/uncategorized/runs/n1m9lna0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220512_135310-n1m9lna0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5p3xoshf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name_or_path: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mist/wandb/run-20220512_135416-5p3xoshf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp_pj/uncategorized/runs/5p3xoshf\" target=\"_blank\">neat-sweep-4</a></strong> to <a href=\"https://wandb.ai/nlp_pj/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv\" target=\"_blank\">https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  16\n",
      "Learning_rate =  3e-05\n",
      "epochs => 4\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     98.    Elapsed: 0:00:04.\n",
      "  Batch    80  of     98.    Elapsed: 0:00:09.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epcoh took: 0:00:11\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.85\n",
      "  Validation Loss: 0.33\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     98.    Elapsed: 0:00:04.\n",
      "  Batch    80  of     98.    Elapsed: 0:00:09.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epcoh took: 0:00:11\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation Loss: 0.28\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     98.    Elapsed: 0:00:04.\n",
      "  Batch    80  of     98.    Elapsed: 0:00:09.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epcoh took: 0:00:11\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 0.21\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     98.    Elapsed: 0:00:04.\n",
      "  Batch    80  of     98.    Elapsed: 0:00:09.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:00:11\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 0.22\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:47 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>avg_val_loss</td><td>█▅▁▂</td></tr><tr><td>train_batch_loss</td><td>█▅▄▅▅▄▄▆▃▇▄▃▂▂▃▅▁▁▅▁▁▁▁▂▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.02052</td></tr><tr><td>avg_val_loss</td><td>0.22342</td></tr><tr><td>train_batch_loss</td><td>0.00162</td></tr><tr><td>val_accuracy</td><td>0.93382</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">neat-sweep-4</strong>: <a href=\"https://wandb.ai/nlp_pj/uncategorized/runs/5p3xoshf\" target=\"_blank\">https://wandb.ai/nlp_pj/uncategorized/runs/5p3xoshf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220512_135416-5p3xoshf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p6o3r6wl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name_or_path: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mist/wandb/run-20220512_135529-p6o3r6wl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp_pj/uncategorized/runs/p6o3r6wl\" target=\"_blank\">bright-sweep-5</a></strong> to <a href=\"https://wandb.ai/nlp_pj/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv\" target=\"_blank\">https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  32\n",
      "Learning_rate =  5e-05\n",
      "epochs => 4\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     49.    Elapsed: 0:00:07.\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epcoh took: 0:00:09\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.88\n",
      "  Validation Loss: 0.27\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     49.    Elapsed: 0:00:07.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epcoh took: 0:00:09\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 0.26\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     49.    Elapsed: 0:00:07.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epcoh took: 0:00:09\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 0.26\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     49.    Elapsed: 0:00:07.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:00:09\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 0.23\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:39 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>avg_val_loss</td><td>█▆▆▁</td></tr><tr><td>train_batch_loss</td><td>▇▆▅█▆▆▄▅▅▅▃▂▂▂▄▄▂▁▆▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.00856</td></tr><tr><td>avg_val_loss</td><td>0.22845</td></tr><tr><td>train_batch_loss</td><td>0.00262</td></tr><tr><td>val_accuracy</td><td>0.94118</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">bright-sweep-5</strong>: <a href=\"https://wandb.ai/nlp_pj/uncategorized/runs/p6o3r6wl\" target=\"_blank\">https://wandb.ai/nlp_pj/uncategorized/runs/p6o3r6wl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220512_135529-p6o3r6wl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rtz7mpqk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name_or_path: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mist/wandb/run-20220512_135631-rtz7mpqk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp_pj/uncategorized/runs/rtz7mpqk\" target=\"_blank\">balmy-sweep-6</a></strong> to <a href=\"https://wandb.ai/nlp_pj/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv\" target=\"_blank\">https://wandb.ai/nlp_pj/uncategorized/sweeps/syunmbgv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  32\n",
      "Learning_rate =  3e-05\n",
      "epochs => 4\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     49.    Elapsed: 0:00:08.\n",
      "\n",
      "  Average training loss: 0.41\n",
      "  Training epcoh took: 0:00:09\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation Loss: 0.39\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     49.    Elapsed: 0:00:07.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epcoh took: 0:00:09\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.89\n",
      "  Validation Loss: 0.28\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     49.    Elapsed: 0:00:07.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epcoh took: 0:00:09\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 0.26\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     49.    Elapsed: 0:00:07.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epcoh took: 0:00:09\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation Loss: 0.24\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:40 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▅▂▁</td></tr><tr><td>avg_val_loss</td><td>█▃▂▁</td></tr><tr><td>train_batch_loss</td><td>█▇▆█▆▇▄▆▆▇▄▄▄▃▄▄▃▃▅▂▂▂▂▃▂▂▂▁▂▁▁▁▁▂▂▃▁▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.045</td></tr><tr><td>avg_val_loss</td><td>0.24091</td></tr><tr><td>train_batch_loss</td><td>0.03164</td></tr><tr><td>val_accuracy</td><td>0.9184</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">balmy-sweep-6</strong>: <a href=\"https://wandb.ai/nlp_pj/uncategorized/runs/rtz7mpqk\" target=\"_blank\">https://wandb.ai/nlp_pj/uncategorized/runs/rtz7mpqk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220512_135631-rtz7mpqk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id,function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run good-sweep-2 with 0.9533582089552238% validation accuracy\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "sweep = api.sweep(\"nlp_pj/uncategorized/syunmbgv\")\n",
    "runs = sorted(sweep.runs,key=lambda run: run.summary.get(\"val_accuracy\", 0), reverse=True)\n",
    "val_acc = runs[0].summary.get(\"val_accuracy\")\n",
    "print(f\"Best run {runs[0].name} with {val_acc}% validation accuracy\")\n",
    "\n",
    "#runs[0].file(\"model.h5\").download(replace=True)\n",
    "#print(\"Best model saved to model-best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(\"nlp_pj/uncategorized\")\n",
    "\n",
    "summary_list, config_list, name_list = [], [], []\n",
    "for run in runs: \n",
    "\n",
    "    summary_list.append(run.summary._json_dict)\n",
    "\n",
    "    config_list.append(\n",
    "        {k: v for k,v in run.config.items()\n",
    "          if not k.startswith('_')})\n",
    "\n",
    "    name_list.append(run.name)\n",
    "\n",
    "runs_df = pd.DataFrame({\n",
    "    \"summary\": summary_list,\n",
    "    \"config\": config_list,\n",
    "    \"name\": name_list\n",
    "    })\n",
    "\n",
    "runs_df.to_csv(\"project.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "#represents a Python iterable over a dataset\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, LRate, train_dataloader, val_dataloader, test_dataloader,num_epoch, gpu):\n",
    "    opti = optim.Adam(model.parameters(), lr = LRate) #,label_smoothing=0.0\n",
    "    best_acc = 0\n",
    "    st = time.time()\n",
    "    cand_list = []\n",
    "    for ep in range(num_epoch):\n",
    "        \n",
    "        model.train()\n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_dataloader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()  \n",
    "            \n",
    "            #Converting these to cuda tensors\n",
    "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "            \n",
    "            #Obtaining the logits from the model\n",
    "            logits = model(seq, attn_masks)\n",
    "\n",
    "            #Computing loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step()\n",
    "              \n",
    "            if it % 100 == 0:\n",
    "                \n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "                st = time.time()\n",
    "\n",
    "        \n",
    "        dev_acc, dev_loss = evaluate(model, criterion, val_dataloader, gpu)\n",
    "        \n",
    "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
    "        if dev_acc > best_acc:\n",
    "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "            best_acc = dev_acc\n",
    "            torch.save(model.state_dict(), 'sstcls_{}.dat'.format(ep))\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        #Iterate over the test_loader \n",
    "        for step, batch in enumerate(test_dataloader):\n",
    "            #Transfer batch to GPUs\n",
    "            batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "            #We dont need to update gradients as we are just predicting\n",
    "            with torch.no_grad():\n",
    "                #Bring up the next batch of input_texts and attention_masks \n",
    "                b_input_ids, b_input_mask = batch\n",
    "                #Forward propogate the inputs and get output as logits\n",
    "\n",
    "                logits = model(b_input_ids, b_input_mask)\n",
    "                #Pass the outputs through a sigmoid function to get the multi-label preditions\n",
    "                s = nn.Sigmoid() \n",
    "                out = s(logits).to('cpu').numpy()\n",
    "                #Add the predictions for this batch to the final list\n",
    "                outputs.extend(out)\n",
    "        cand_list.append(outputs)\n",
    "    return cand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp\n",
    "import torch.nn.functional as F\n",
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1)) \n",
    "    #probs = F.softmax(logits, dim=0)\n",
    "    preds = (probs > 0.5).long()\n",
    "    acc = f1_score(preds.squeeze().detach().cpu().numpy(), labels.detach().cpu().numpy(), average = 'micro')\n",
    "    return acc\n",
    "\n",
    "def evaluate(model, criterion, dataloader, gpu):\n",
    "    model.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "            logits = model(seq, attn_masks)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the rumor detection classifier, initialised with pretrained BERT-BASE parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the rumor detection classifier.\n"
     ]
    }
   ],
   "source": [
    "gpu = 0 #gpu ID\n",
    "\n",
    "print(\"Creating the rumor detection classifier, initialised with pretrained BERT-BASE parameters...\")\n",
    "model = RumorDetectionClassifier()\n",
    "model.cuda(gpu) #Enable gpu support for the model\n",
    "print(\"Done creating the rumor detection classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 0.6060468554496765; Accuracy: 0.75; Time taken (s): 0.08098936080932617\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.10284268856048584; Accuracy: 1.0; Time taken (s): 6.957259654998779\n",
      "Epoch 0 complete! Development Accuracy: 0.9123134328358209; Development Loss: 0.21881516344511687\n",
      "Best development accuracy improved from 0 to 0.9123134328358209, saving model...\n",
      "Iteration 0 of epoch 1 complete. Loss: 0.1182466596364975; Accuracy: 1.0; Time taken (s): 10.432116985321045\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.03237377852201462; Accuracy: 1.0; Time taken (s): 7.826496124267578\n",
      "Epoch 1 complete! Development Accuracy: 0.9421641791044776; Development Loss: 0.14780610948523035\n",
      "Best development accuracy improved from 0.9123134328358209 to 0.9421641791044776, saving model...\n",
      "Iteration 0 of epoch 2 complete. Loss: 0.010133877396583557; Accuracy: 1.0; Time taken (s): 10.863122701644897\n",
      "Iteration 100 of epoch 2 complete. Loss: 0.008637635037302971; Accuracy: 1.0; Time taken (s): 7.133285045623779\n",
      "Epoch 2 complete! Development Accuracy: 0.9552238805970149; Development Loss: 0.1442445787816628\n",
      "Best development accuracy improved from 0.9421641791044776 to 0.9552238805970149, saving model...\n",
      "Iteration 0 of epoch 3 complete. Loss: 0.01009313017129898; Accuracy: 1.0; Time taken (s): 9.916627407073975\n",
      "Iteration 100 of epoch 3 complete. Loss: 0.0021386544685810804; Accuracy: 1.0; Time taken (s): 5.8765833377838135\n",
      "Epoch 3 complete! Development Accuracy: 0.9589552238805971; Development Loss: 0.16322444418771886\n",
      "Best development accuracy improved from 0.9552238805970149 to 0.9589552238805971, saving model...\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 4\n",
    "LRate = 0.00003\n",
    "preds_fine_tuning = train(model, criterion, LRate, train_dataloader, val_dataloader, test_dataloader,num_epoch, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.frame import DataFrame\n",
    "from collections import Counter\n",
    "def MajorityVoting(outputs):\n",
    "    preds_fine_tuning_df = DataFrame(outputs)\n",
    "    def probToLabel(num):\n",
    "        prob = float(np.asarray(num))\n",
    "        if prob > 0.5:\n",
    "            #print(prob)\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    new_df = preds_fine_tuning_df.applymap(probToLabel)\n",
    "    preds_ensembling = []\n",
    "    for name, values in new_df.iteritems():\n",
    "        #print(new_df[name])\n",
    "        preds_ensembling.append(Counter(list(new_df[name])).most_common(1)[0][0])\n",
    "    return preds_ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ensembling = MajorityVoting(preds_fine_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [i for i in range(len(preds_ensembling))]\n",
    "predicted_df = pd.DataFrame({'Id':id_list,'Predicted':preds_ensembling})\n",
    "predicted_df.to_csv(\"test.predicted.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = train_input_ids.tolist()+dev_input_ids.tolist()\n",
    "#input_ids =  np.array(input_ids)\n",
    "labels = train_labels+dev_labels\n",
    "attn_masks = train_attn_masks.tolist()+dev_attn_masks.tolist()\n",
    "#attn_masks = np.array(attn_masks)\n",
    "preprocess_df = pd.DataFrame({'input_ids':input_ids,'labels':labels,'attn_masks':attn_masks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the rumor detection classifier, initialised with pretrained BERT-BASE parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the rumor detection classifier.\n"
     ]
    }
   ],
   "source": [
    "gpu = 0 #gpu ID\n",
    "\n",
    "print(\"Creating the rumor detection classifier, initialised with pretrained BERT-BASE parameters...\")\n",
    "model = RumorDetectionClassifier()\n",
    "model.cuda(gpu) #Enable gpu support for the model\n",
    "print(\"Done creating the rumor detection classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 0.8297592401504517; Accuracy: 0.0; Time taken (s): 0.07875561714172363\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.36297494173049927; Accuracy: 1.0; Time taken (s): 7.358769178390503\n",
      "Epoch 0 complete! Development Accuracy: 0.8370253164556962; Development Loss: 0.38422293128752255\n",
      "Best development accuracy improved from 0 to 0.8370253164556962, saving model...\n",
      "Iteration 0 of epoch 1 complete. Loss: 0.3603743314743042; Accuracy: 0.75; Time taken (s): 9.718173503875732\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.38923266530036926; Accuracy: 0.875; Time taken (s): 6.983335971832275\n",
      "Epoch 1 complete! Development Accuracy: 0.9108649789029535; Development Loss: 0.18514114872941487\n",
      "Best development accuracy improved from 0.8370253164556962 to 0.9108649789029535, saving model...\n",
      "Iteration 0 of epoch 2 complete. Loss: 0.009328015148639679; Accuracy: 1.0; Time taken (s): 9.098792314529419\n",
      "Iteration 100 of epoch 2 complete. Loss: 0.004731050226837397; Accuracy: 1.0; Time taken (s): 7.00451135635376\n",
      "Epoch 2 complete! Development Accuracy: 0.9256329113924051; Development Loss: 0.2652675081523359\n",
      "Best development accuracy improved from 0.9108649789029535 to 0.9256329113924051, saving model...\n",
      "Iteration 0 of epoch 3 complete. Loss: 0.0017094560898840427; Accuracy: 1.0; Time taken (s): 8.72013258934021\n",
      "Iteration 100 of epoch 3 complete. Loss: 0.0031398111023008823; Accuracy: 1.0; Time taken (s): 6.181762456893921\n",
      "Epoch 3 complete! Development Accuracy: 0.9287974683544303; Development Loss: 0.3106783224734672\n",
      "Best development accuracy improved from 0.9256329113924051 to 0.9287974683544303, saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 0.0015140671748667955; Accuracy: 1.0; Time taken (s): 0.08220672607421875\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.07042383402585983; Accuracy: 1.0; Time taken (s): 7.3735411167144775\n",
      "Epoch 0 complete! Development Accuracy: 0.9778481012658228; Development Loss: 0.08180485971572608\n",
      "Best development accuracy improved from 0 to 0.9778481012658228, saving model...\n",
      "Iteration 0 of epoch 1 complete. Loss: 0.016465017572045326; Accuracy: 1.0; Time taken (s): 9.669538497924805\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.189011350274086; Accuracy: 0.875; Time taken (s): 6.994989395141602\n",
      "Epoch 1 complete! Development Accuracy: 0.9762658227848101; Development Loss: 0.1050199047723841\n",
      "Iteration 0 of epoch 2 complete. Loss: 0.015816837549209595; Accuracy: 1.0; Time taken (s): 8.41226840019226\n",
      "Iteration 100 of epoch 2 complete. Loss: 0.0007060837233439088; Accuracy: 1.0; Time taken (s): 6.950765371322632\n",
      "Epoch 2 complete! Development Accuracy: 0.9825949367088608; Development Loss: 0.07708699831553394\n",
      "Best development accuracy improved from 0.9778481012658228 to 0.9825949367088608, saving model...\n",
      "Iteration 0 of epoch 3 complete. Loss: 0.002583915600553155; Accuracy: 1.0; Time taken (s): 8.996232509613037\n",
      "Iteration 100 of epoch 3 complete. Loss: 0.0004038162878714502; Accuracy: 1.0; Time taken (s): 7.637427568435669\n",
      "Epoch 3 complete! Development Accuracy: 0.9794303797468354; Development Loss: 0.10187782140288802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 0.000347429740941152; Accuracy: 1.0; Time taken (s): 0.07765364646911621\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.002340195933356881; Accuracy: 1.0; Time taken (s): 5.889510869979858\n",
      "Epoch 0 complete! Development Accuracy: 0.9984177215189873; Development Loss: 0.007714324469436431\n",
      "Best development accuracy improved from 0 to 0.9984177215189873, saving model...\n",
      "Iteration 0 of epoch 1 complete. Loss: 0.0031031863763928413; Accuracy: 1.0; Time taken (s): 8.088213920593262\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.0009498874424025416; Accuracy: 1.0; Time taken (s): 7.394997835159302\n",
      "Epoch 1 complete! Development Accuracy: 0.9968354430379747; Development Loss: 0.014176146583734787\n",
      "Iteration 0 of epoch 2 complete. Loss: 0.008857407607138157; Accuracy: 1.0; Time taken (s): 8.886963367462158\n",
      "Iteration 100 of epoch 2 complete. Loss: 0.00034928671084344387; Accuracy: 1.0; Time taken (s): 7.150412321090698\n",
      "Epoch 2 complete! Development Accuracy: 0.9984177215189873; Development Loss: 0.008376347741017802\n",
      "Iteration 0 of epoch 3 complete. Loss: 0.0003636169130913913; Accuracy: 1.0; Time taken (s): 8.913968324661255\n",
      "Iteration 100 of epoch 3 complete. Loss: 0.0003522238985169679; Accuracy: 1.0; Time taken (s): 7.015865325927734\n",
      "Epoch 3 complete! Development Accuracy: 0.9984177215189873; Development Loss: 0.009137540726603416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 0.000390275614336133; Accuracy: 1.0; Time taken (s): 0.08238983154296875\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.00019720390264410526; Accuracy: 1.0; Time taken (s): 7.299767017364502\n",
      "Epoch 0 complete! Development Accuracy: 0.9968354430379747; Development Loss: 0.020779271934367863\n",
      "Best development accuracy improved from 0 to 0.9968354430379747, saving model...\n",
      "Iteration 0 of epoch 1 complete. Loss: 0.00015537664876319468; Accuracy: 1.0; Time taken (s): 9.715476989746094\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.003326812759041786; Accuracy: 1.0; Time taken (s): 7.129087686538696\n",
      "Epoch 1 complete! Development Accuracy: 1.0; Development Loss: 0.0010241193485393224\n",
      "Best development accuracy improved from 0.9968354430379747 to 1.0, saving model...\n",
      "Iteration 0 of epoch 2 complete. Loss: 0.0002811567683238536; Accuracy: 1.0; Time taken (s): 9.642694234848022\n",
      "Iteration 100 of epoch 2 complete. Loss: 0.00019914793665520847; Accuracy: 1.0; Time taken (s): 6.991206169128418\n",
      "Epoch 2 complete! Development Accuracy: 1.0; Development Loss: 0.0006882895787741659\n",
      "Iteration 0 of epoch 3 complete. Loss: 0.000172003434272483; Accuracy: 1.0; Time taken (s): 8.6421217918396\n",
      "Iteration 100 of epoch 3 complete. Loss: 0.00019669995526783168; Accuracy: 1.0; Time taken (s): 7.381846189498901\n",
      "Epoch 3 complete! Development Accuracy: 1.0; Development Loss: 0.0005053814765451995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 0.0001671163336141035; Accuracy: 1.0; Time taken (s): 0.08664584159851074\n",
      "Iteration 100 of epoch 0 complete. Loss: 4.024722147732973e-05; Accuracy: 1.0; Time taken (s): 6.381785869598389\n",
      "Epoch 0 complete! Development Accuracy: 1.0; Development Loss: 0.00013716650062610536\n",
      "Best development accuracy improved from 0 to 1.0, saving model...\n",
      "Iteration 0 of epoch 1 complete. Loss: 0.0001391946861986071; Accuracy: 1.0; Time taken (s): 9.480836629867554\n",
      "Iteration 100 of epoch 1 complete. Loss: 4.9083217163570225e-05; Accuracy: 1.0; Time taken (s): 7.009309768676758\n",
      "Epoch 1 complete! Development Accuracy: 0.9984177215189873; Development Loss: 0.008798643705942388\n",
      "Iteration 0 of epoch 2 complete. Loss: 6.627772381762043e-05; Accuracy: 1.0; Time taken (s): 8.199063777923584\n",
      "Iteration 100 of epoch 2 complete. Loss: 6.15393728367053e-05; Accuracy: 1.0; Time taken (s): 7.739902973175049\n",
      "Epoch 2 complete! Development Accuracy: 0.9984177215189873; Development Loss: 0.009124066774244126\n",
      "Iteration 0 of epoch 3 complete. Loss: 4.576041828840971e-05; Accuracy: 1.0; Time taken (s): 8.879407405853271\n",
      "Iteration 100 of epoch 3 complete. Loss: 5.109458288643509e-05; Accuracy: 1.0; Time taken (s): 6.953240633010864\n",
      "Epoch 3 complete! Development Accuracy: 1.0; Development Loss: 0.00020611784665434935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 5.881279139430262e-05; Accuracy: 1.0; Time taken (s): 0.08019757270812988\n",
      "Iteration 100 of epoch 0 complete. Loss: 6.242530071176589e-05; Accuracy: 1.0; Time taken (s): 7.203251838684082\n",
      "Epoch 0 complete! Development Accuracy: 0.9978902953586497; Development Loss: 0.017623243876521054\n",
      "Best development accuracy improved from 0 to 0.9978902953586497, saving model...\n",
      "Iteration 0 of epoch 1 complete. Loss: 2.4124678020598367e-05; Accuracy: 1.0; Time taken (s): 9.680160999298096\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.0003397318359930068; Accuracy: 1.0; Time taken (s): 7.044120788574219\n",
      "Epoch 1 complete! Development Accuracy: 1.0; Development Loss: 0.00016628697817453296\n",
      "Best development accuracy improved from 0.9978902953586497 to 1.0, saving model...\n",
      "Iteration 0 of epoch 2 complete. Loss: 2.7447556931292638e-05; Accuracy: 1.0; Time taken (s): 9.574346780776978\n",
      "Iteration 100 of epoch 2 complete. Loss: 5.8124227507505566e-05; Accuracy: 1.0; Time taken (s): 7.023404598236084\n",
      "Epoch 2 complete! Development Accuracy: 0.9968354430379747; Development Loss: 0.0056029682147041805\n",
      "Iteration 0 of epoch 3 complete. Loss: 0.0001456429745303467; Accuracy: 1.0; Time taken (s): 8.191543340682983\n",
      "Iteration 100 of epoch 3 complete. Loss: 3.880140502587892e-05; Accuracy: 1.0; Time taken (s): 7.7999536991119385\n",
      "Epoch 3 complete! Development Accuracy: 0.9968354430379747; Development Loss: 0.006531871414107961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 2.1755457055405714e-05; Accuracy: 1.0; Time taken (s): 0.09182095527648926\n",
      "Iteration 100 of epoch 0 complete. Loss: 6.213764208951034e-06; Accuracy: 1.0; Time taken (s): 7.257682800292969\n",
      "Epoch 0 complete! Development Accuracy: 0.9857594936708861; Development Loss: 0.13784790555982449\n",
      "Best development accuracy improved from 0 to 0.9857594936708861, saving model...\n",
      "Iteration 0 of epoch 1 complete. Loss: 4.872666977462359e-06; Accuracy: 1.0; Time taken (s): 9.688417434692383\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.00015050446381792426; Accuracy: 1.0; Time taken (s): 6.942434072494507\n",
      "Epoch 1 complete! Development Accuracy: 1.0; Development Loss: 8.224865853697315e-05\n",
      "Best development accuracy improved from 0.9857594936708861 to 1.0, saving model...\n",
      "Iteration 0 of epoch 2 complete. Loss: 5.32552003278397e-05; Accuracy: 1.0; Time taken (s): 9.550647974014282\n",
      "Iteration 100 of epoch 2 complete. Loss: 6.396789103746414e-05; Accuracy: 1.0; Time taken (s): 7.0239386558532715\n",
      "Epoch 2 complete! Development Accuracy: 0.9889240506329114; Development Loss: 0.035970987371197395\n",
      "Iteration 0 of epoch 3 complete. Loss: 1.05439031124115; Accuracy: 0.875; Time taken (s): 8.37151837348938\n",
      "Iteration 100 of epoch 3 complete. Loss: 0.00017392847803421319; Accuracy: 1.0; Time taken (s): 6.9966607093811035\n",
      "Epoch 3 complete! Development Accuracy: 1.0; Development Loss: 8.078310356377418e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 1.5869609342189506e-05; Accuracy: 1.0; Time taken (s): 0.09055423736572266\n",
      "Iteration 100 of epoch 0 complete. Loss: 2.2500730665342417e-06; Accuracy: 1.0; Time taken (s): 6.961956262588501\n",
      "Epoch 0 complete! Development Accuracy: 1.0; Development Loss: 1.0311975279076746e-06\n",
      "Best development accuracy improved from 0 to 1.0, saving model...\n",
      "Iteration 0 of epoch 1 complete. Loss: 1.564620674798789e-06; Accuracy: 1.0; Time taken (s): 9.295673370361328\n",
      "Iteration 100 of epoch 1 complete. Loss: 1.1473887298052432e-06; Accuracy: 1.0; Time taken (s): 7.103535413742065\n",
      "Epoch 1 complete! Development Accuracy: 1.0; Development Loss: 5.029297956096781e-07\n",
      "Iteration 0 of epoch 2 complete. Loss: 7.450577186318696e-07; Accuracy: 1.0; Time taken (s): 9.029592275619507\n",
      "Iteration 100 of epoch 2 complete. Loss: 5.662439548359544e-07; Accuracy: 1.0; Time taken (s): 6.919949531555176\n",
      "Epoch 2 complete! Development Accuracy: 1.0; Development Loss: 2.639454118672825e-07\n",
      "Iteration 0 of epoch 3 complete. Loss: 4.0233129539046786e-07; Accuracy: 1.0; Time taken (s): 8.849061727523804\n",
      "Iteration 100 of epoch 3 complete. Loss: 2.831220058396866e-07; Accuracy: 1.0; Time taken (s): 6.916813135147095\n",
      "Epoch 3 complete! Development Accuracy: 1.0; Development Loss: 1.4473616000224888e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 2.9802316703353426e-07; Accuracy: 1.0; Time taken (s): 0.08135986328125\n",
      "Iteration 100 of epoch 0 complete. Loss: 1.1920928244535389e-07; Accuracy: 1.0; Time taken (s): 7.667430877685547\n",
      "Epoch 0 complete! Development Accuracy: 1.0; Development Loss: 1.886222823502435e-10\n",
      "Best development accuracy improved from 0 to 1.0, saving model...\n",
      "Iteration 0 of epoch 1 complete. Loss: 2.9802320611338473e-08; Accuracy: 1.0; Time taken (s): 9.675573825836182\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 6.958543062210083\n",
      "Epoch 1 complete! Development Accuracy: 1.0; Development Loss: 0.0\n",
      "Iteration 0 of epoch 2 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 8.228141784667969\n",
      "Iteration 100 of epoch 2 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 7.266549110412598\n",
      "Epoch 2 complete! Development Accuracy: 1.0; Development Loss: 0.0\n",
      "Iteration 0 of epoch 3 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 8.89340615272522\n",
      "Iteration 100 of epoch 3 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 6.968483209609985\n",
      "Epoch 3 complete! Development Accuracy: 1.0; Development Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 0.0856316089630127\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 6.9781529903411865\n",
      "Epoch 0 complete! Development Accuracy: 1.0; Development Loss: 0.0\n",
      "Best development accuracy improved from 0 to 1.0, saving model...\n",
      "Iteration 0 of epoch 1 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 9.04682183265686\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 7.098207712173462\n",
      "Epoch 1 complete! Development Accuracy: 1.0; Development Loss: 0.0\n",
      "Iteration 0 of epoch 2 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 8.894661664962769\n",
      "Iteration 100 of epoch 2 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 7.159968614578247\n",
      "Epoch 2 complete! Development Accuracy: 1.0; Development Loss: 0.0\n",
      "Iteration 0 of epoch 3 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 9.0114266872406\n",
      "Iteration 100 of epoch 3 complete. Loss: 0.0; Accuracy: 1.0; Time taken (s): 6.97410774230957\n",
      "Epoch 3 complete! Development Accuracy: 1.0; Development Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "def train_validate_test_split(df, train_percent=.7, validate_percent=.3, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    train = df.loc[perm[:train_end]]\n",
    "    validate = df.loc[perm[train_end:]]\n",
    "    return train, validate\n",
    "\n",
    "cand_list = []\n",
    "for r in range(10):\n",
    "    en_train, en_validate = train_validate_test_split(preprocess_df)\n",
    "    en_X_train = torch.tensor(list(en_train['input_ids']))\n",
    "    en_X_val = torch.tensor(list(en_validate['input_ids']))    \n",
    "    en_y_train = torch.tensor(list(en_train['labels']))    \n",
    "    en_y_val = torch.tensor(list(en_validate['labels']))    \n",
    "    en_attention_masks_train = torch.tensor(list(en_train['attn_masks']))\n",
    "    en_attention_masks_val = torch.tensor(list(en_validate['attn_masks']))    \n",
    "    en_test_input_ids = torch.tensor(test_input_ids)\n",
    "    en_test_attention_masks = torch.tensor(test_attn_masks)\n",
    " \n",
    "    en_train_data = torch.utils.data.TensorDataset(en_X_train, en_attention_masks_train, en_y_train)\n",
    "    en_val_data = torch.utils.data.TensorDataset(en_X_val, en_attention_masks_val, en_y_val)\n",
    "    en_test_data = torch.utils.data.TensorDataset(en_test_input_ids, en_test_attention_masks)\n",
    "\n",
    "    en_train_sampler = torch.utils.data.RandomSampler(en_train_data)\n",
    "    en_val_sampler = torch.utils.data.SequentialSampler(en_val_data)\n",
    "    en_test_sampler = torch.utils.data.SequentialSampler(en_test_data)\n",
    "    \n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "    en_train_dataloader = DataLoader(en_train_data, sampler = en_train_sampler, batch_size = BATCH_SIZE)\n",
    "    en_val_dataloader = DataLoader(en_val_data, sampler = en_val_sampler, batch_size = BATCH_SIZE)\n",
    "    en_test_dataloader = DataLoader(en_test_data, sampler = en_test_sampler, batch_size = BATCH_SIZE)\n",
    "    \n",
    "    num_epoch = 4\n",
    "    LRate = 3e-5\n",
    "    \n",
    "    opti = optim.Adam(model.parameters(), lr = LRate) #,label_smoothing=0.0\n",
    "    best_acc = 0\n",
    "    st = time.time()\n",
    "    \n",
    "    for ep in range(num_epoch):\n",
    "        \n",
    "        model.train()\n",
    "        for it, (seq, attn_masks, labels) in enumerate(en_train_dataloader):\n",
    "\n",
    "            opti.zero_grad()  \n",
    "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "            logits = model(seq, attn_masks)\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "              \n",
    "            if it % 100 == 0:\n",
    "                \n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "                st = time.time()\n",
    "\n",
    "        \n",
    "        dev_acc, dev_loss = evaluate(model, criterion, en_val_dataloader, gpu)\n",
    "        \n",
    "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
    "        if dev_acc > best_acc:\n",
    "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "            best_acc = dev_acc\n",
    "            torch.save(model.state_dict(), 'sstcls_{}.dat'.format(ep))\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    " \n",
    "        for step, batch in enumerate(en_test_dataloader):\n",
    "            batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                b_input_ids, b_input_mask = batch\n",
    "\n",
    "                logits = model(b_input_ids, b_input_mask)\n",
    "                s = nn.Sigmoid() \n",
    "                out = s(logits).to('cpu').numpy()\n",
    "                outputs.extend(out)\n",
    "        cand_list.append(outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sample_ensembling = MajorityVoting(cand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list_sample = [i for i in range(len(preds_sample_ensembling))]\n",
    "predicted_df_sample = pd.DataFrame({'Id':id_list_sample,'Predicted':preds_sample_ensembling})\n",
    "predicted_df_sample.to_csv(\"test.predicted_sample.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK2: COVID-19 Rumor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_input_ids, covid_labels, covid_attn_masks = preprocessing(dataset_type=\"covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_input_ids = torch.tensor(covid_input_ids)\n",
    "covid_attention_masks = torch.tensor(covid_attn_masks)\n",
    "covid_data = torch.utils.data.TensorDataset(covid_input_ids, covid_attention_masks)\n",
    "covid_sampler = torch.utils.data.SequentialSampler(covid_data)\n",
    "covid_dataloader = DataLoader(covid_data, sampler = covid_sampler, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_events, covid_line_index= readData(dataset_type=\"covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1496"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(covid_line_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_pred_index_df = pd.DataFrame({'predicted_index':covid_line_index})\n",
    "covid_pred_index_df.to_csv(\"covid_predicted_index.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RumorDetectionClassifier(\n",
       "  (bert_layer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls_layer): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu = 0 #gpu ID\n",
    "model = RumorDetectionClassifier()\n",
    "model.cuda(gpu) #Enable gpu support for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: 0.7182886004447937; Accuracy: 0.5; Time taken (s): 0.10550618171691895\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.2919047474861145; Accuracy: 0.875; Time taken (s): 8.79378628730774\n",
      "Epoch 0 complete! Development Accuracy: 0.9011194029850746; Development Loss: 0.23004813803665675\n",
      "Best development accuracy improved from 0 to 0.9011194029850746, saving model...\n",
      "Iteration 0 of epoch 1 complete. Loss: 0.18369704484939575; Accuracy: 1.0; Time taken (s): 40.253735065460205\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.11061564087867737; Accuracy: 0.875; Time taken (s): 7.688759088516235\n",
      "Epoch 1 complete! Development Accuracy: 0.8899253731343284; Development Loss: 0.2271653250026614\n",
      "Iteration 0 of epoch 2 complete. Loss: 0.24670301377773285; Accuracy: 0.875; Time taken (s): 40.18888020515442\n",
      "Iteration 100 of epoch 2 complete. Loss: 0.0017059332458302379; Accuracy: 1.0; Time taken (s): 8.287480592727661\n",
      "Epoch 2 complete! Development Accuracy: 0.957089552238806; Development Loss: 0.14823437282475016\n",
      "Best development accuracy improved from 0.9011194029850746 to 0.957089552238806, saving model...\n",
      "Iteration 0 of epoch 3 complete. Loss: 0.002392373513430357; Accuracy: 1.0; Time taken (s): 41.694666147232056\n",
      "Iteration 100 of epoch 3 complete. Loss: 0.002311933785676956; Accuracy: 1.0; Time taken (s): 7.572626829147339\n",
      "Epoch 3 complete! Development Accuracy: 0.9533582089552238; Development Loss: 0.18070822339348577\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 4\n",
    "LRate = 0.00003\n",
    "covid_preds = train(model, criterion, LRate, train_dataloader, val_dataloader, covid_dataloader,num_epoch, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_preds_ensembling = MajorityVoting(covid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_id_list = [i for i in range(len(covid_preds_ensembling))]\n",
    "covid_preds_df = pd.DataFrame({'Id':covid_id_list,'Predicted':covid_preds_ensembling})\n",
    "covid_preds_df.to_csv(\"covid_predictions.csv\",index=False,sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
