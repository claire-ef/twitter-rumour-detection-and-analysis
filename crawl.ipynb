{"cells":[{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["import tweepy\n","import json"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# set up api\n","consumer_key = 'N66nZbZ4ijAoLZhjrrBoUJCTX'\n","consumer_secret = 'ieXgfvG3hpG9Q0Yrwg2R1othTB4kYUcC2fVMrqGXXm4y2hwqUS'\n","bearer_token = 'AAAAAAAAAAAAAAAAAAAAAM6IcAEAAAAAClcPlojryXpI8shOO99W%2FDPJczU%3Dq4vfJ3Oun0k6ZZwJxGO8PTlXKSHv3fpinMDzagEqE59xsMgLd2'\n","access_token = '1518751356840181760-X5Zwz6UrQivZ2Wq23LgKy3ng4vMM2w'\n","access_token_secret = '1f1N0ogVglz8BU3Dpm4Z6rYFu1bHi4G9FxniLXF2MXL30'\n","\n","auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n","api = tweepy.API(auth)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["lines = []\n","with open('project-data/train' + '.data.txt') as f:\n","    for line in f.readlines():\n","        lines.append(line.rstrip('\\n').split(','))\n","#for s in api.lookup_statuses(lines[0]):\n","    # print(s._json)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["7003\n"]}],"source":["# crawl train, dev tweets\n","fail = 0\n","for data in ['train', 'dev']:\n","    with open('project-data/' + data + '.data.txt') as f:\n","        for line in f.readlines():\n","            ids = line.rstrip('\\n').split(',')\n","            try:\n","                tweets = api.lookup_statuses(ids)\n","                for tweet, id in zip(tweets, ids):\n","                    with open('project-data/crawled_tweets/' + data + '/' + id + '.json', 'w+') as fp:\n","                        json.dump(tweet._json, fp)\n","            except:\n","                for id in ids:\n","                    try:\n","                        tweet = api.get_status(int(id))\n","                        with open('project-data/crawled_tweets/' + data + '/' + id + '.json', 'w+') as fp:\n","                            json.dump(tweet._json, fp)\n","                    except:\n","                        fail += 1\n","                        continue\n","                continue\n","print(fail)\n","            "]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["85511\n"]}],"source":["# crawl train, dev tweets\n","fail = 0\n","for data in ['train', 'dev', 'covid']:\n","    with open('project-data/' + data + '.data.txt') as f:\n","        for line in f.readlines():\n","            ids = line.rstrip('\\n').split(',')\n","            try:\n","                tweets = api.lookup_statuses(ids)\n","                for tweet, id in zip(tweets, ids):\n","                    with open('project-data/crawled_tweets/' + data + '/' + id + '.json', 'w+') as fp:\n","                        json.dump(tweet._json, fp)\n","            except:\n","                for id in ids:\n","                    try:\n","                        tweet = api.get_status(int(id))\n","                        with open('project-data/crawled_tweets/' + data + '/' + id + '.json', 'w+') as fp:\n","                            json.dump(tweet._json, fp)\n","                    except:\n","                        fail += 1\n","                        continue\n","                continue\n","print(fail)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["28633\n","10172\n","7757\n"]}],"source":["for data in ['train', 'dev', 'test', 'covid']:\n","    ids = set()\n","    with open('project-data/' + data + '.data.txt') as f:\n","        for line in f.readlines():\n","            ids |= set(line.rstrip('\\n').split(','))\n","    print(len(ids))"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"ename":"TooManyRequests","evalue":"429 Too Many Requests\n88 - Rate limit exceeded","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-3d2da0c9f89c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_L\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_L\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_statuses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'project-data/crawled_tweets/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'covid'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mlookup_statuses\u001b[0;34m(self, id, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m                 \u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'include_entities'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trim_user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'map'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                 \u001b[0;34m'include_ext_alt_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'include_card_uri'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             ), id=list_to_csv(id), **kwargs\n\u001b[0m\u001b[1;32m    662\u001b[0m         )\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m429\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTooManyRequests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTwitterServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTooManyRequests\u001b[0m: 429 Too Many Requests\n88 - Rate limit exceeded"]}],"source":["# crawl covid tweets\n","with open('project-data/covid.data.txt') as f:\n","    ids_L = []\n","    for line in f.readlines():\n","        ids_L += line.rstrip('\\n').split(',')\n","for i in range(1, len(ids_L)//100 + 2):\n","    if i <= len(ids_L)//100:\n","        ids = ids_L[(i-1)*100:i*100]\n","        tweets = api.lookup_statuses(ids)\n","        for tweet, id in zip(tweets, ids):\n","            with open('project-data/crawled_tweets/' + 'covid' + '/' + id + '.json', 'w+') as fp:\n","                json.dump(tweet._json, fp)\n","    else:\n","        ids = ids_L[(i-1)*100:]\n","        tweets = api.lookup_statuses(ids)\n","        for tweet, id in zip(tweets, ids):\n","            with open('project-data/crawled_tweets/' + 'covid' + '/' + id + '.json', 'w+') as fp:\n","                json.dump(tweet._json, fp)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/plain":["1895"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["# crawl covid tweets\n","with open('project-data/train.data.txt') as f:\n","    ids_L = []\n","    for line in f.readlines():\n","        ids_L.append(line.rstrip('\\n').split(','))\n","len(ids_L)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["632"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["with open('project-data/dev.data.txt') as f:\n","    ids_L = []\n","    for line in f.readlines():\n","        ids_L.append(line.rstrip('\\n').split(','))\n","len(ids_L)"]}],"metadata":{"interpreter":{"hash":"40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"},"kernelspec":{"display_name":"Python 3.7.6 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
